{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_type = 'PCA' #ICA\n",
    "n_components = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pre-combined right hemisphere vertex files\n",
    "vertex_data_right = pd.read_csv(\"filename.csv\", sep=',', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pre-combined left hemisphere vertex files\n",
    "vertex_data_left = pd.read_csv(\"filename.csv\", sep=',', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score each feature matrix individually\n",
    "vertex_data_valid_scaled_right = StandardScaler().fit_transform(vertex_data_right)\n",
    "vertex_data_valid_scaled_left = StandardScaler().fit_transform(vertex_data_left)\n",
    "#print(vertex_data_valid_scaled_left.shape)\n",
    "#print(np.max(vertex_data_valid_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine left and right hemispheres\n",
    "vertex_data_valid_scaled_full = np.concatenate((vertex_data_valid_scaled_left,vertex_data_valid_scaled_right),axis=1)\n",
    "#print(vertex_data_valid_scaled_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score each subject individually\n",
    "vertex_data_valid_scaled_full = StandardScaler().fit_transform(np.transpose(vertex_data_valid_scaled_full))\n",
    "vertex_data_valid_scaled_full = np.transpose(vertex_data_valid_scaled_full)\n",
    "#print(vertex_data_valid_scaled_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup for ICA or PCA \n",
    "if decomp_type == 'PCA':\n",
    "    comp_model = PCA(n_components=n_components)\n",
    "\n",
    "if decomp_type == 'ICA':\n",
    "    comp_model = FastICA(n_components=n_components)\n",
    "    \n",
    "all_components = comp_model.fit_transform(vertex_data_valid_scaled_full)#.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = comp_model.components_\n",
    "numweights = all_weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: no explained variance for ICA do not run\n",
    "plt.plot(list(range(0,n_components)),comp_model.explained_variance_ratio_)\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "print(comp_model.explained_variance_ratio_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What PC is at the 80% explained variance threshold\n",
    "n_comps_80p = 0\n",
    "n_comps_80p_i = 0\n",
    "while n_comps_80p < .8:\n",
    "    n_comps_80p = n_comps_80p + comp_model.explained_variance_ratio_[n_comps_80p_i]\n",
    "    n_comps_80p_i = n_comps_80p_i + 1\n",
    "print(str(n_comps_80p_i) + \" : \" + str(n_comps_80p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export components to single excel file\n",
    "component_file_location = r'..\\filename' + decomp_type +'_'+str(n_components)+'.csv'\n",
    "np.savetxt(component_file_location, all_components, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export component weights to individual files\n",
    "for ix in range(0,all_weights.shape[0]):\n",
    "    filename_temp_lr = r'..\\filename_loading_left_' + + decomp_type + \"_\" + str(ix+1) + '.txt'\n",
    "    np.savetxt(filename_temp_lr, all_weights[ix,:numweights//2], delimiter='\\n')\n",
    "    filename_temp_lr = r'..\\filename_loading_right_' + str(ix+1) + '.txt'\n",
    "    np.savetxt(filename_temp_lr, all_weights[ix,numweights//2:], delimiter='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
